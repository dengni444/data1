{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.1\n",
      "12.1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)    #查看cpu版本\n",
    "print(torch.version.cuda)\n",
    "     #查看gpu版本\n",
    "from torch_geometric.data import HeteroData\n",
    "from torch_geometric.datasets import OGB_MAG\n",
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''多重图：friend边、visit边、revisit边、co-occ共现边、live-with边、class边\n",
    "理论上都是有向图\n",
    "有的是对称边，有的是非对称边，对称边本代码中只记录其中一半\n",
    "把一天分为四段 22-5 5-10 10-15 15-22  需要embedding\n",
    "friend边是对称边，边特征定义为  meeting次数在四个不同时间段，和家之间的距离 特征数5\n",
    "visit边是非对称边，边特征定义为  四个不同时间段分别的访问次数  无则为填0  特征数4\n",
    "revisit边是非对称边，边特征定义为 四个不同时间段分别的访问次数  无则为填0  特征数4 与visit边对称 但是起始点和终点的node type不一样\n",
    "co-occ共现边是对称边 如果两个poi在一个用户的历史轨迹中出现，则设为1，多出现了几次，则+n, 以及两点之间的距离 特征数2\n",
    "live-with是如果poi在用户的家🏠旁边1.5KM,设为1 特征数1\n",
    "re_live_with边\n",
    "class边 是对称边，  poi上的类别数最多位为3                               如果两个poi是同类，则设为1, 两个点之间的距离  特征数2\n",
    "'''\n",
    "from torch_geometric.data import HeteroData\n",
    "from torch_geometric.datasets import OGB_MAG\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import math\n",
    "def distance(origin, destination):  # 计算经纬度点之间的距离\n",
    "    lat1, lon1 = origin\n",
    "    lat2, lon2 = destination\n",
    "    radius = 6371  # km\n",
    "    dlat = math.radians(lat2 - lat1)\n",
    "    dlon = math.radians(lon2 - lon1)\n",
    "    a = math.sin(dlat / 2) * math.sin(dlat / 2) + math.cos(math.radians(lat1)) \\\n",
    "        * math.cos(math.radians(lat2)) * math.sin(dlon / 2) * math.sin(dlon / 2)\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "    d = radius * c\n",
    "    return d  # 返回的是km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "city=\"TKY\"\n",
    "path=\"FourSquare/Data_cut_city/follow_mat/\"+city+\"/\"\n",
    "\n",
    "check_in=pd.read_csv(path+\"checkins_\"+city.lower()+\"_follow_mat.csv\")\n",
    "poi_data=pd.read_csv(path+\"POI_\"+city.lower()+\"_follow_mat.csv\")   #目前存在的问题是check_in的用户少\n",
    "friend_old=pd.read_csv(\"dataset_WWW2019/dataset_WWW_friendship_old.txt\",sep='\\t',header=None)\n",
    "friend_new=pd.read_csv(\"dataset_WWW2019/dataset_WWW_friendship_new.txt\",sep='\\t',header=None)\n",
    "friend_old.columns = ['user1','user2']\n",
    "friend_new.columns = ['user1','user2']\n",
    "\n",
    "\n",
    "print(check_in)\n",
    "print(poi_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_check_in=set(list(check_in['userid']))  #从check_in出来，找到check_in之后再去找user和friend_ship   3754\n",
    "print(\"一共有\",len(user_check_in),\"个user存在check_in\")\n",
    "\n",
    "print(user_check_in)\n",
    "\n",
    "\n",
    "venue_check_in=set(list(check_in['Venue_id']))   #从check_in出来 找POI\n",
    "print(\"一共有\",len(venue_check_in),\"个Venue存在check_in\")\n",
    "\n",
    "friend=pd.concat([friend_old,friend_new]) #把new和old结合起来\n",
    "friend.drop_duplicates(subset=['user1','user2'],keep='first',inplace=True)\n",
    "friend = friend.reset_index(drop=True)  #重新设置索引  #701317 rows × 2 columns 全量的friendship关系\n",
    "\n",
    "friend_list=[] #\n",
    "for index,row in friend.iterrows():\n",
    "    if row['user1'] in user_check_in and row['user2'] in user_check_in:\n",
    "        if row['user1']!=row['user2']: #朋友关系边 两边不能是同一个人\n",
    "            friend_list.append([row['user1'],row['user2']])\n",
    "#         friend_list.append([row['user2'],row['user1']])#对称边\n",
    "\n",
    "#print(friend_list)\n",
    "\n",
    "print(\"user之间一共有\",len(friend_list),\"条边\")\n",
    "user=[]\n",
    "for i in range(len(friend_list)):\n",
    "    user.append(friend_list[i][0])\n",
    "    user.append(friend_list[i][1])\n",
    "print(\"一共有\",len(set(user)),\"个user存在好友关系\")\n",
    "print(user)\n",
    "print(\"有\",len(user_check_in)-len(set(user)),\"个自闭症患者，不和这3811  个用户之间存在好友关系\")\n",
    "print(friend_list[:10]) #就是原始的朋友关系对"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#建立索引\n",
    "'''建立索引，使用字典''' '''再转成矩阵形式'''  # 用户 user需要索引，poi也需要索引\n",
    "import os\n",
    "\n",
    "\n",
    "user_check_in_sort = list(sorted(list(user_check_in)))  # TKY 用户表\n",
    "# print(user_check_in_sort)\n",
    "\n",
    "user_dic = {}  # {用户id: user_index}  用户的索引\n",
    "for i in range(len(user_check_in_sort)):\n",
    "    user_dic[user_check_in_sort[i]] = i\n",
    "\n",
    "    \n",
    "    \n",
    "   #########\n",
    "a=set(list(poi_data['category']))   #从check_in出来 找POI\n",
    "A = list(sorted(list(a)))  # TKY 用户表\n",
    "# # print(user_check_in_sort)\n",
    "AA_CAT = {}  # {用户id: user_index}  用户的索引\n",
    "for i in range(len(A)):\n",
    "    AA_CAT[A[i]] = i\n",
    "\n",
    "\n",
    "print(AA_CAT)\n",
    "########\n",
    "    \n",
    "    \n",
    "    \n",
    "# print(user_dic)\n",
    "    \n",
    "    \n",
    "venue_check_in_sort = list(sorted(list(venue_check_in)))  # TKY POI表\n",
    "Venue_id_dic = {}  # {POIid: poi_index} POI索引\n",
    "for i in range(len(venue_check_in_sort)):\n",
    "    Venue_id_dic[venue_check_in_sort[i]] = i\n",
    "    \n",
    "# print(Venue_id_dic)\n",
    "\n",
    "friend_list_index = []  # 好友关系{user1_index：user2_index} 好友 #对称\n",
    "for i in range(len(friend_list)):\n",
    "    friend_list_index.append((user_dic[friend_list[i][0]], user_dic[friend_list[i][1]]))\n",
    "print(friend_list_index[:10])\n",
    "\n",
    "\n",
    "# 文件保存路径\n",
    "save_path = \"data/Multi-dimensional_Graphs/\" + city\n",
    "\n",
    "# 确保目录存在，如果不存在则创建\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "# 保存好友关系索引\n",
    "np.save(os.path.join(save_path, \"friend_list_index.npy\"), torch.tensor(np.array(friend_list_index)).t().contiguous())\n",
    "#print( torch.tensor(np.array(friend_list_index)).t().contiguous())\n",
    "#np.save(\"data/Multi-dimensional_Graphs/\"+city+\"/friend_list_index.npy\",torch.tensor(np.array(friend_list_index)).t().contiguous())  #保存 friend list边"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "a=set(list(poi_data['category']))   #从check_in出来 找POI\n",
    "A = list(sorted(list(a)))  # TKY 用户表\n",
    "# # print(user_check_in_sort)\n",
    "AA_CAT = {}  # {种类id: user_index}  用户的索引\n",
    "for i in range(len(A)):\n",
    "    AA_CAT[A[i]] = i\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#修改poi——data\n",
    "poi_data['Venue_id_index']=None\n",
    "poi_data['category_index']=None\n",
    "for index, row in poi_data.iterrows():  #把poi数据中的Venueid换成poi索引\n",
    "    poi_data['Venue_id_index'][index]=Venue_id_dic[row['Venue_id']]\n",
    "    poi_data['category_index'][index]=AA_CAT[row['category']]\n",
    "    \n",
    "    \n",
    "    \n",
    "    if 'Caf' in row['category']:\n",
    "        if len(row['category'])==5 or len(row['category'])==4 :\n",
    "            poi_data['category'][index]='caf\\u00e9'    \n",
    "poi_data=poi_data.sort_values('Venue_id', ascending=True, inplace=False )\n",
    "poi_data=poi_data.reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "print(poi_data)\n",
    "\n",
    "\n",
    "poi_lat_lon={} #{poi_index:(lat,lon)}\n",
    "for index, row in poi_data.iterrows(): \n",
    "    poi_lat_lon[index]=(row['latitude'],row['longitude'])\n",
    "   \n",
    "\n",
    "cat={} #{poi_index:(lat,lon)}\n",
    "cat1={}\n",
    "for index, row in poi_data.iterrows(): \n",
    "    cat[index]=row['category']\n",
    "    cat1[index]=row['category_index']\n",
    "\n",
    "# poi_data\n",
    "#print(poi_data)\n",
    "#print(poi_lat_lon)\n",
    "\n",
    "#修改check-in数据\n",
    "def time_fenge(hour): #取值区间为[0,23]  把一天分为4段 \n",
    "    #把一天分为四段 22-5 5-10 10-15 15-22  需要embedding\n",
    "    if 5<=hour<10:\n",
    "        return 1\n",
    "    elif 10<=hour<15:\n",
    "        return 2\n",
    "    elif 15<=hour<22:\n",
    "        return 3\n",
    "    elif 22<=hour or hour<5 :\n",
    "        return 0\n",
    "\n",
    "import time\n",
    "check_in=check_in.sort_values('Venue_id', ascending=True, inplace=False ) #将原始的checkin根据poi排序\n",
    "check_in=check_in.reset_index(drop=True)\n",
    "check_in['unix_time']=None  #把check in 中的utc时间变成unix时间戳 未加时区\n",
    "check_in['userid_index']=None \n",
    "check_in['Venue_id_index']=None\n",
    "check_in['loacl_time_year']=None\n",
    "check_in['loacl_time_mon']=None #月 #月份（从一月开始，0代表一月） - 取值区间为[0,11] */\n",
    "check_in['loacl_time_mday']=None #一个月中的日期 - 取值区间为[1,31] */\n",
    "check_in['loacl_time_hour']=None #时 - 取值区间为[0,23]\n",
    "check_in['loacl_time_min']=None #分 - 取值区间为[0,59]\n",
    "check_in['loacl_time_sec']=None\n",
    "check_in['loacl_time_wday']=None  #星期 – 取值区间为[0,6]，其中0代表星期一，1代表星期二，以此类推 */\n",
    "check_in['hour_periods']=None\n",
    "check_in['lat_lon']=None\n",
    "\n",
    "\n",
    "check_in_unix_time=[]\n",
    "check_in_userid_index=[]\n",
    "check_in_Venue_id_index=[]\n",
    "check_in_loacl_time_year=[]\n",
    "check_in_loacl_time_mon=[]\n",
    "check_in_loacl_time_mday=[]\n",
    "check_in_loacl_time_hour=[]\n",
    "check_in_loacl_time_min=[]\n",
    "check_in_loacl_time_sec=[]\n",
    "check_in_loacl_time_wday=[]\n",
    "check_in_hour_periods=[]\n",
    "check_in_lat_lon=[]\n",
    "\n",
    "\n",
    "catt=[]\n",
    "\n",
    "catt22=[]\n",
    "for index,row in check_in.iterrows():\n",
    "    time_1=row['utc_time'][:-10]+row['utc_time'][-4:]\n",
    "    Timezone_offset=row['Timezone_offset']\n",
    "    struct_time = time.mktime(time.strptime(time_1, \"%a %b %d  %H:%M:%S %Y\"))+((Timezone_offset))*60 #当地时间戳 已经加上了时区信息 \n",
    "    localtime = time.localtime(struct_time)  #返回元组\n",
    "    \n",
    "    \n",
    "    check_in_unix_time.append(struct_time)\n",
    "\n",
    "    check_in_loacl_time_year.append(localtime[0])\n",
    "    check_in_loacl_time_mon.append(localtime[1])\n",
    "    check_in_loacl_time_mday.append(localtime[2])\n",
    "    check_in_loacl_time_hour.append(localtime[3]) #小时\n",
    "    check_in_loacl_time_min.append(localtime[4])\n",
    "    check_in_loacl_time_sec.append(localtime[5])\n",
    "    check_in_loacl_time_wday.append(localtime[6])\n",
    "    check_in_hour_periods.append(time_fenge(localtime[3]))\n",
    "\n",
    "    check_in_userid_index.append(user_dic[row['userid']])\n",
    "    check_in_Venue_id_index.append(Venue_id_dic[row['Venue_id']])\n",
    "    check_in_lat_lon.append( poi_lat_lon[Venue_id_dic[row['Venue_id']]])\n",
    "    \n",
    "    \n",
    "    catt.append( cat[Venue_id_dic[row['Venue_id']]])\n",
    "    \n",
    "    catt22.append( cat1[Venue_id_dic[row['Venue_id']]])\n",
    "    \n",
    "    \n",
    "check_in['unix_time'] =check_in_unix_time\n",
    "check_in['userid_index']=check_in_userid_index\n",
    "check_in['Venue_id_index']=check_in_Venue_id_index\n",
    "check_in['loacl_time_year']=check_in_loacl_time_year\n",
    "check_in['loacl_time_mon']=check_in_loacl_time_mon #月 #月份（从一月开始，0代表一月） - 取值区间为[0,11] */\n",
    "check_in['loacl_time_mday']=check_in_loacl_time_mday #一个月中的日期 - 取值区间为[1,31] */\n",
    "check_in['loacl_time_hour']=check_in_loacl_time_hour #时 - 取值区间为[0,23]\n",
    "check_in['loacl_time_min']=check_in_loacl_time_min #分 - 取值区间为[0,59]\n",
    "check_in['loacl_time_sec']=check_in_loacl_time_sec\n",
    "check_in['loacl_time_wday']=check_in_loacl_time_wday \n",
    "check_in['hour_periods']=check_in_hour_periods\n",
    "check_in['lat_lon']=check_in_lat_lon\n",
    "\n",
    "\n",
    "check_in['category']=catt\n",
    "check_in['category_id']=catt22\n",
    "\n",
    "# 假设你要从 DataFrame 中删除名为 'column_name' 的列\n",
    "check_in.drop(columns=['cat_index'], inplace=True)\n",
    "\n",
    "check_in\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_in\n",
    "\n",
    "check_in.to_csv('FourSquare/Data_cut_city/follow_mat/TKY/df_tky_check_in_cid.csv',encoding=\"utf_8_sig\",index=False) #保存东京checkin_根据mat数据确定user和venueid  # 698889 rows × 4 columns\n",
    "poi_data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 假设你要将 'old_column_name' 更改为 'new_column_name'\n",
    "poi_data.rename(columns={'category_index': 'category_id'}, inplace=True)\n",
    "poi_data\n",
    "poi_data.to_csv('FourSquare/Data_cut_city/follow_mat/TKY/df_tky_poi_data_cid.csv',encoding=\"utf_8_sig\",index=False) #保存东京checkin_根据mat数据确定user和venueid  # 698889 rows × 4 columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_in\n",
    "\n",
    "#check_in.to_csv('FourSquare/Data_cut_city/follow_mat/TKY/df_tky_check_in.csv',encoding=\"utf_8_sig\",index=False) #保存东京checkin_根据mat数据确定user和venueid  # 698889 rows × 4 columns\n",
    "poi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poi_data\n",
    "  # 原来有7232 实际上7219\n",
    "#poi_data.to_csv('FourSquare/Data_cut_city/follow_mat/TKY/df_tky_poi_data.csv',encoding=\"utf_8_sig\",index=False) #保存东京checkin_根据mat数据确定user和venueid  # 698889 rows × 4 columns\n",
    "len(set(list(poi_data['category'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "visit_list = []\n",
    "\n",
    "for index, row in check_in.iterrows():\n",
    "    visit_list.append((row['userid_index'],row['Venue_id_index']))\n",
    "    # visit_list.append((user_dic[row['userid']], Venue_id_dic[row['Venue_id']]))\n",
    "print(visit_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把签到关系 转成边 user 和 poi的边 有向图\n",
    "# visit边是非对称边，边特征定义为  四个不同时间段分别的访问次数  无则为填0  特征数4\n",
    "# revisit边是非对称边，边特征定义为 四个不同时间段分别的访问次数  无则为填0  特征数4 与visit边对称 但是起始点和终点的node type不一样\n",
    "\n",
    "def get_index(lst=None, item=''):  #Python从列表中找出所有元素索引的方法\n",
    "    return [index for (index,value) in enumerate(lst) if value == item]\n",
    "\n",
    "visit_list = []\n",
    "\n",
    "for index, row in check_in.iterrows():\n",
    "    visit_list.append((row['userid_index'],row['Venue_id_index']))\n",
    "    # visit_list.append((user_dic[row['userid']], Venue_id_dic[row['Venue_id']]))\n",
    "    \n",
    "visit_list_edge=list(sorted(list(set(visit_list))))   #看一共有多少个访问序列的边\n",
    "re_visit_list_edge=[]\n",
    "for (user,poi) in visit_list_edge:\n",
    "    re_visit_list_edge.append((poi,user)) #revisit切换起始与重点的值\n",
    "\n",
    "visit_list_hour_per=[]\n",
    "for index, row in check_in.iterrows():\n",
    "    visit_list_hour_per.append((row['userid_index'],row['Venue_id_index'],row['hour_periods']))\n",
    "\n",
    "#用户访问POI的边的特征  用户的历史访问次数\n",
    "visit_list_edge_attr=[]\n",
    "\n",
    "for i in range(len(visit_list_edge)):\n",
    "    index_=get_index(visit_list,visit_list_edge[i])\n",
    "    cishu=len(index_) #总次数\n",
    "    edge_hour=[0,0,0,0]\n",
    "    for j in index_:\n",
    "        a=visit_list_hour_per[j][2]\n",
    "        edge_hour[a]+=1  #统计每个时间段的次数\n",
    "#     cishu=visit_list.count(visit_list_edge[i]) #不需要总次数了\n",
    "#     visit_list_edge_attr.append(cishu) #do not need any more\n",
    "    visit_list_edge_attr.append(edge_hour)\n",
    "re_visit_list_edge_attr=visit_list_edge_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"data/Multi-dimensional_Graphs/\"+city+\"/visit_list_edge_tensor.npy\",torch.tensor(np.array(visit_list_edge)).t().contiguous()) #user -> visit -> poi 单向边\n",
    "np.save(\"data/Multi-dimensional_Graphs/\"+city+\"/visit_list_edge_attr.npy\",torch.tensor(np.array(visit_list_edge_attr))) #user -> visit -> poi 单向边特征 不同时间段的访问次数\n",
    "np.save(\"data/Multi-dimensional_Graphs/\"+city+\"/revisit_list_edge_tensor.npy\",torch.tensor(np.array(re_visit_list_edge)).t().contiguous()) #poi -> revisit -> user 单向边\n",
    "np.save(\"data/Multi-dimensional_Graphs/\"+city+\"/revisit_list_edge_attr.npy\",torch.tensor(np.array(re_visit_list_edge_attr))) #poi -> revisit -> user 单向边特征 不同时间段的访问次数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_in2 = check_in.sort_values('userid_index', ascending=True, inplace=False)  # 将原始的checkin根据poi排序\n",
    "group = check_in2.groupby(\"userid_index\")  # 分组\n",
    "group = list(group)\n",
    "print(group[0])\n",
    "user_home={} #用户家的住址\n",
    "for i in range(len(group)):\n",
    "    userid=group[i][0]\n",
    "    df = pd.DataFrame(group[i][1])\n",
    "    df = df.reset_index(drop=True)\n",
    "    user_loc=df['lat_lon']\n",
    "    lat=[]\n",
    "    lon=[]\n",
    "    for (lat1,lon1) in list(user_loc):\n",
    "        lat.append(lat1)\n",
    "        lon.append(lon1)\n",
    "    lat_aver=np.mean(lat) #user的签到点的平均为家住宅\n",
    "    lon_aver=np.mean(lon)\n",
    "    user_home[i]=(lat_aver,lon_aver)\n",
    "user_home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''本段代码运行需要14分钟'''\n",
    "#friend 边的meeting关系 \n",
    "#friend边是对称边，边特征定义为  meeting次数在四个不同时间段，和家之间的距离 特征数5\n",
    "group = check_in.groupby(\"Venue_id\")   #分组\n",
    "group=list(group)\n",
    "meeting_Threshold=1800*2  # 单位秒 s 共0.5h 1h  1800s是0.5h \n",
    "#1h 算见面的阈值\n",
    "friend_list_dict_attr={} \n",
    "for i in range(len(friend_list_index)):\n",
    "#     print(friend_list)\n",
    "    index_friend=str(friend_list_index[i][0]) + str(\"+\")+str(friend_list_index[i][1])\n",
    "    friend_list_dict_attr[index_friend]=[0,0,0,0,0] #初始化 5维特征\n",
    "    \n",
    "for i in range(len(group)):\n",
    "#     print(i)  \n",
    "    df=pd.DataFrame(group[i][1])  #每一个group是一个访问过同一poi的不同用户的人\n",
    "    df=df.reset_index(drop=True)\n",
    "    for index,row in df.iterrows():\n",
    "        for index2,row2 in df.iterrows():\n",
    "            if index !=index2:\n",
    "                if abs(row['unix_time']-row2['unix_time'])< meeting_Threshold: #两个人在0.5小时内相聚在同一POI\n",
    "                    if (row['userid_index'],row2['userid_index']) in friend_list_index : #如果两个人见面了并且他们是朋友\n",
    "                        hour_per=row['hour_periods'] \n",
    "                        friend_list_dict_attr[str(row['userid_index'])+str(\"+\")+str(row2['userid_index'])][hour_per]+=1\n",
    "                    elif (row2['userid_index'],row['userid_index']) in friend_list_index:  #friend_list_index 实际上是双向边，但是这里为了方便只保存了单向的结果\n",
    "                        hour_per=row['hour_periods'] \n",
    "                        friend_list_dict_attr[str(row2['userid_index'])+str(\"+\")+str(row['userid_index'])][hour_per]+=1\n",
    "    \n",
    "friend_edge_attr=[]\n",
    "for i in range(len(friend_list_index)): #把user_index版本的特征按照输入data中的边的顺序重组\n",
    "    dis=distance(user_home[friend_list_index[i][0]] , user_home[friend_list_index[i][1]]) \n",
    "    friend_list_dict_attr[str(friend_list_index[i][0]) + str(\"+\")+str(friend_list_index[i][1])][4]=dis  #计算两个user家之间的距离\n",
    "    \n",
    "    friend_edge_attr.append(friend_list_dict_attr[str(friend_list_index[i][0])+str(\"+\")+str(friend_list_index[i][1])])\n",
    "    \n",
    "friend_edge_attr= (np.array(friend_edge_attr))\n",
    "print(friend_edge_attr)\n",
    "np.save(\"data/Multi-dimensional_Graphs/\"+city+\"/friend_edge_attr.npy\",friend_edge_attr)  #特征数5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''poi 到user 还有live_with关系，如果poi距离user的家近 1.5km''' \n",
    "def POi_live_with_user(poi_data,user_home):  #poi 到user的边，poi在前\n",
    "    live_with = []\n",
    "    live_with_attr=[]\n",
    "    re_live_with = []\n",
    "    re_live_with_attr=[]\n",
    "    for i in range(len(\tuser_home.keys())):\n",
    "        (lat_user,lon_user)=user_home[i] #user的签到点的平均为家住宅\n",
    "        for index, row in poi_data.iterrows():\n",
    "            dis=distance((lat_user,lon_user),(row['latitude'],row['longitude']))\n",
    "            if dis<=1.5 : #1.5公里以内\n",
    "                live_with.append((row['Venue_id_index'],i)) # poi 到user的边，poi在前\n",
    "                re_live_with.append((i,row['Venue_id_index'])) # user 到poi 的边，user在前\n",
    "                live_with_attr.append(dis)\n",
    "                re_live_with_attr.append(dis)\n",
    "    live_with=np.array(live_with)\n",
    "    re_live_with=np.array(re_live_with)\n",
    "#     live_with_attr=np.ones((live_with.shape[0],1))\n",
    "    live_with=torch.tensor(live_with,dtype=torch.long).t().contiguous()\n",
    "    re_live_with=torch.tensor(re_live_with,dtype=torch.long).t().contiguous()\n",
    "    live_with_attr=np.array(live_with_attr).reshape(-1,1)\n",
    "    re_live_with_attr=np.array(re_live_with_attr).reshape(-1,1)\n",
    "    return live_with,live_with_attr,re_live_with,re_live_with_attr\n",
    "\n",
    "live_with_edge,live_with_attr,re_live_with_edge,re_live_with_attr =POi_live_with_user(poi_data,user_home)\n",
    "np.save(\"../data/Multi-dimensional_Graphs/\"+city+\"/live_with_edge.npy\",live_with_edge)\n",
    "np.save(\"../data/Multi-dimensional_Graphs/\"+city+\"/live_with_attr.npy\",live_with_attr)\n",
    "np.save(\"../data/Multi-dimensional_Graphs/\"+city+\"/re_live_with_edge.npy\",re_live_with_edge)\n",
    "np.save(\"../data/Multi-dimensional_Graphs/\"+city+\"/re_live_with_attr.npy\",re_live_with_attr)\n",
    "live_with_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''poi 与poi 之间的关系 共现关系与同类关系'''\n",
    "#先处理同类关系 \n",
    "def POi_class_same_POI(poi_data):  #poi 与poi同类即可有边    直接存储的就是双向边，\n",
    "    class_same_edge=[]\n",
    "    class_same_edge_attr=[]\n",
    "    for index, row in poi_data.iterrows():\n",
    "        print(index)\n",
    "        for index2,row2 in poi_data.iterrows():\n",
    "            \n",
    "            if index!=index2:\n",
    "#                 if (row['cat0_0']==row2['cat0_0']) or (row['cat0_1']==row2['cat0_1']) or (row['cat0_2']==row2['cat0_2']) \\\n",
    "#                 or (row['cat1_0']==row2['cat1_0']) or (row['cat1_1']==row2['cat1_1']) or (row['cat1_2']==row2['cat1_2']) \\\n",
    "#                 or (row['cat2_0']==row2['cat2_0']) or (row['cat2_1']==row2['cat2_1']) or (row['cat2_2']==row2['cat2_2']):\n",
    "                if  (row['cat0_1']==row2['cat0_1']) or (row['cat0_2']==row2['cat0_2']) \\\n",
    "                 or (row['cat1_1']==row2['cat1_1']) or (row['cat1_2']==row2['cat1_2']) \\\n",
    "                or (row['cat2_1']==row2['cat2_1']) or (row['cat2_2']==row2['cat2_2']):\n",
    "                    \n",
    "                    x1=int(row['cat0_0']==row2['cat0_0'] )\n",
    "                    x2=int(row['cat0_1']==row2['cat0_1'] )\n",
    "                    x3=int(row['cat0_2']==row2['cat0_2'] )\n",
    "                    x4=int(row['cat1_0']==row2['cat1_0'] )\n",
    "                    x5=int(row['cat1_1']==row2['cat1_1'] )\n",
    "                    x6=int(row['cat1_2']==row2['cat1_2'] )\n",
    "                    x7=int(row['cat2_0']==row2['cat2_0'] )\n",
    "                    x8=int(row['cat2_1']==row2['cat2_1'] )\n",
    "                    x9=int(row['cat2_2']==row2['cat2_2'] )\n",
    "                    class_same_edge.append((index,index2))  #双向边 \n",
    "                    class_same_edge_attr.append((x1,x2,x3,x4,x5,x6,x7,x8,x9,   distance((row['latitude'],row['longitude']),(row2['latitude'],row2['longitude']))))\n",
    "\n",
    "    class_same_edge=np.array(class_same_edge)\n",
    "    class_same_edge=torch.tensor(class_same_edge,dtype=torch.long).t().contiguous()\n",
    "    return  class_same_edge, class_same_edge_attr\n",
    "\n",
    "class_same_edge,class_same_edge_attr=POi_class_same_POI(poi_data)\n",
    "np.save(\"../data/Multi-dimensional_Graphs/\"+city+\"/class_same_edge.npy\",class_same_edge)\n",
    "np.save(\"../data/Multi-dimensional_Graphs/\"+city+\"/class_same_edge_attr.npy\",class_same_edge_attr)\n",
    "\n",
    "class_same_edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''共现边初始化'''\n",
    "co_occurrence_list_index=[]  #全链接 用来做后面的poi-poi之间边的特征  有向图，就算共线 也要考虑出现顺序\n",
    "for i in range(poi_data.shape[0]):\n",
    "    for j in range(poi_data.shape[0]):\n",
    "        if i ==j:\n",
    "            continue\n",
    "        else:\n",
    "#             if (i,j) in co_occurrence_list_index or (j,i) in co_occurrence_list_index:\n",
    "#                 continue\n",
    "#             else:\n",
    "            co_occurrence_list_index.append((i,j))\n",
    "\n",
    "co_occurrence_list_index=sorted(list(set(co_occurrence_list_index))) #只是用于初始化 后续要删掉没有共现的poi边\n",
    "\n",
    "\n",
    "co_occurrence_list_dict_attr={} \n",
    "for (u_poi,v_poi) in co_occurrence_list_index:\n",
    "    index_co_occ=str(u_poi)+str(\"+\")+str(v_poi)\n",
    "    co_occurrence_list_dict_attr[index_co_occ]=[0,0] #co_occ边的特征 有两个 共线次数与poi之间的距离\n",
    "\n",
    "# print(co_occurrence_list_dict_attr)\n",
    "co_occurrence_list_index[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##统计两两节点之间的共现次数   总比例为0.001792570896019172  \n",
    "check_in=check_in.sort_values('userid_index', ascending=True, inplace=False )  #将原始的checkin根据poi排序\n",
    "group2 = check_in.groupby(\"userid_index\")   #分组\n",
    "group2=list(group2)\n",
    "for i in range(len(group2)):                 #每一个group2 都是用户\n",
    "    if i%300==0:\n",
    "        print(i)\n",
    "    df=pd.DataFrame(group2[i][1])            #每个用户的访问轨迹\n",
    "    df=df.reset_index(drop=True)\n",
    "    df_venu=sorted(set(list(df['Venue_id_index'])) ) #去重  poi列表\n",
    "    df_venu_times={}                    #记录一个poi在一个用户轨迹中出现的次数 \n",
    "    for i in range(len(df_venu)):\n",
    "        df_venu_times[df_venu[i]]=(list(df['Venue_id_index'])).count(df_venu[i])\n",
    "\n",
    "    for i in range(len(df_venu)):\n",
    "        for  j in range(i+1,len(df_venu)):\n",
    "            poi_poi=df_venu_times[df_venu[i]]*df_venu_times[df_venu[j]]\n",
    "            co_occurrence_list_dict_attr[str(i)+\"+\"+str(j)][0]+=poi_poi\n",
    "            co_occurrence_list_dict_attr[str(j)+\"+\"+str(i)][0]+=poi_poi\n",
    "\n",
    "co_occurrence_list_dict_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co_occurrence_list_dict_attr2={} #保存只存在共现关系的边\n",
    "for x in co_occurrence_list_dict_attr.keys():\n",
    "    if co_occurrence_list_dict_attr[x][0]!=0: \n",
    "        co_occurrence_list_dict_attr2[x]= co_occurrence_list_dict_attr[x]\n",
    "for x in co_occurrence_list_dict_attr2.keys():\n",
    "    x2=x.split(\"+\", 1)\n",
    "    u=int(x2[0])\n",
    "    v=int(x2[-1])\n",
    "#     print((u,v))\n",
    "    dis=distance((poi_data.loc[u]['latitude'],poi_data.loc[u]['longitude']),(poi_data.loc[v]['latitude'],poi_data.loc[v]['longitude']))\n",
    "    co_occurrence_list_dict_attr2[x][1]=dis\n",
    "co_occurrence_list_dict_attr2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co_occurrence_list_index2=[]\n",
    "co_occurrence_list_dict_attr3=[]\n",
    "for x in co_occurrence_list_dict_attr2.keys():\n",
    "    \n",
    "    x2=x.split(\"+\", 1)\n",
    "    attr=co_occurrence_list_dict_attr2[x]\n",
    "    co_occurrence_list_index2.append((int(x2[0]),int(x2[1])))\n",
    "    co_occurrence_list_dict_attr3.append(attr)\n",
    "# co_occurrence_list_index2\n",
    "\n",
    "np.save(\"data/Multi-dimensional_Graphs/\"+city+\"/co_occurrence_list_index.npy\",torch.tensor(np.array(co_occurrence_list_index2)).t().contiguous())\n",
    "np.save(\"data/Multi-dimensional_Graphs/\"+city+\"/co_occurrence_attr.npy\",co_occurrence_list_dict_attr3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dgl",
   "language": "python",
   "name": "dgl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
